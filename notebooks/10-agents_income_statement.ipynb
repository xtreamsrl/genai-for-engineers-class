{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents to parse and comment an income statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will try and use agents to download, parse, and comment on an income statement.\n",
    "\n",
    "The document is real, and it is publicly available on the internet at the time of writing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: packages and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "\n",
    "if not importlib.util.find_spec(\"class_utils\"):\n",
    "    !pip install -qqq git+https://github.com/xtreamsrl/genai-for-engineers-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWRZeaVJS3LA",
    "outputId": "24292119-8a0f-49de-f137-a18eb0dcdb47"
   },
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating agents\n",
    "\n",
    "For this purpose, we will use the latest model, GPT-4o.\n",
    "\n",
    "Less advanced models are less effective in writing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"config_list\": [{\"model\": \"gpt-4o\"}],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Proxy\n",
    "UserProxyAgent is a special type of agent that can be used to proxy user input to another agent or group of agents. It supports the following human input modes:\n",
    "- ALWAYS: Always ask user for input.\n",
    "- NEVER: Never ask user for input. In this mode, the agent will use the default response (if any) to respond to the message. Or using underlying LLM model to generate response if provided.\n",
    "- AUTO: Only ask user for input when conversation is terminated by the other agent(s). Otherwise, use the default response (if any) to respond to the message. Or using underlying LLM model to generate response if provided.\n",
    "\n",
    "We will execute the code natively without docker. This is a security flaw, and in real life it should be avoided, as the execution of arbitrary code can expose sensitive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"income_statement_analysis\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    "    human_input_mode=\"NEVER\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define roles\n",
    "\n",
    "It's your turn now. Define the agents you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_chat = autogen.GroupChat(\n",
    "    agents=[user_proxy], messages=[], max_round=30, send_introductions=True\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=group_chat,\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the chat\n",
    "Let's trigger the grup chat and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.cisiaonline.it/_mamawp/wp-content/uploads/2015/07/Bilancio-esercizio_Nota-integr_CISIA-2017.pdf\"\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=f\"At this link is an income statement: {link}. \"\n",
    "    \"Please evaluate it, considering it is written in Italian.\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

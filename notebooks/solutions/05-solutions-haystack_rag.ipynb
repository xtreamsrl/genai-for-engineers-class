{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait, RAG again?\n",
    "In the notebook 03, we implemented a RAG pipeline from scratch, using only the Qdrant and the OpenAI SDK.\n",
    "Now, we want to build something similar using Haystack. Once agiain, we expect to get a more readable and maintanable code, at the expense of taking on an extra dependency, and one that will forever be entangled in our application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: packages and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "if not importlib.util.find_spec(\"class_utils\"):\n",
    "    !pip install -qqq git+https://github.com/xtreamsrl/genai-for-engineers-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWRZeaVJS3LA",
    "outputId": "24292119-8a0f-49de-f137-a18eb0dcdb47"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from haystack import Pipeline, Document\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.embedders import (\n",
    "    SentenceTransformersTextEmbedder,\n",
    "    SentenceTransformersDocumentEmbedder,\n",
    ")\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.document_stores.types import DocumentStore\n",
    "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
    "from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ...\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a pipeline to create embeddings\n",
    "\n",
    "The first step is to embed documents. We'll use an the `InMemoryDocumentStore`, an in-memory structure that is a much simplified version of a vector database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        content=\"Poor Things is a 2023 film directed by Yorgos Lanthimos and written by Tony McNamara, \"\n",
    "        \"based on the 1992 novel by Alasdair Gray.\"\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"Oppenheimer is a 2023 epic biographical thriller film[a] written, directed,\"\n",
    "        \" and co-produced by Christopher Nolan.[8] It follows the life of J. Robert \"\n",
    "        \"Oppenheimer, the American theoretical physicist who helped develop the \"\n",
    "        \"first nuclear weapons during World War II\"\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"Dune: Part Two is a 2024 American epic science fiction film directed and produced by Denis \"\n",
    "        \"Villeneuve, who co-wrote the screenplay with Jon Spaihts. The sequel to Dune (2021), it \"\n",
    "        \"is the second of a two-part adaptation of the 1965 novel Dune by Frank Herbert. \"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_indexing_pipline(\n",
    "    document_store: DocumentStore,\n",
    "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    ") -> Pipeline:\n",
    "    pipe = Pipeline()\n",
    "    pipe.add_component(\n",
    "        instance=SentenceTransformersDocumentEmbedder(model=embedding_model),\n",
    "        name=\"doc_embedder\",\n",
    "    )\n",
    "    pipe.add_component(\n",
    "        instance=DocumentWriter(document_store=document_store), name=\"doc_writer\"\n",
    "    )\n",
    "    pipe.connect(\"doc_embedder.documents\", \"doc_writer.documents\")\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWeZFFXAVzup"
   },
   "outputs": [],
   "source": [
    "document_store = InMemoryDocumentStore()\n",
    "indexing_pipeline = build_indexing_pipline(document_store)\n",
    "indexing_pipeline.run({\"doc_embedder\": {\"documents\": documents}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the documents are there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.filter_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline\n",
    "\n",
    "Great, now we can buid the proper RAG pipeline using our documents.\n",
    "As in notebook 02, we need a prompt template. However, this time we will use a real templating engine, [Jinja](https://jinja.palletsprojects.com/en/3.1.x/). \n",
    "\n",
    "We will implement our RAG as a Pipiline. Pipelines are the key abstraction of Haystack (and Langchain, and Llamaindex). \n",
    "\n",
    "The pipelines in Haystack 2.0 are directed multigraphs of different Haystack components and integrations. They give the freedom to connect these components in various ways. This means that the pipeline doesn't need to be a continuous stream of information. With the flexibility of Haystack pipelines, you can have simultaneous flows, standalone components, loops, and other types of connections.\n",
    "\n",
    "Learn more at https://docs.haystack.deepset.ai/docs/pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the questions based on the given context.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "Question: {{ question }}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_openai_rag_pipeline(\n",
    "    retriever: InMemoryEmbeddingRetriever | QdrantEmbeddingRetriever,\n",
    "    prompt_template: str,\n",
    "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    openai_model: str = \"gpt-3.5-turbo\",\n",
    ") -> Pipeline:\n",
    "    pipe = Pipeline()\n",
    "    pipe.add_component(\n",
    "        \"embedder\", SentenceTransformersTextEmbedder(model=embedding_model)\n",
    "    )\n",
    "    pipe.add_component(\"retriever\", retriever)\n",
    "    pipe.add_component(\n",
    "        \"prompt_builder\",\n",
    "        PromptBuilder(template=prompt_template, required_variables=\"*\"),\n",
    "    )\n",
    "    pipe.add_component(\"llm\", OpenAIGenerator(model=openai_model))\n",
    "\n",
    "    pipe.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
    "    pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "    pipe.connect(\"prompt_builder\", \"llm\")\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipe = build_openai_rag_pipeline(\n",
    "    InMemoryEmbeddingRetriever(document_store), template\n",
    ")\n",
    "rag_pipe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "query = \"What film talks about the atomic bomb?\"\n",
    "response = rag_pipe.run(\n",
    "    {\"embedder\": {\"text\": query}, \"prompt_builder\": {\"question\": query}}\n",
    ")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline with Qdrant\n",
    "\n",
    "Now we'll build the same pipeline with Qdrant. \n",
    "\n",
    "Once again, the components in Haystack will help us perform the update quickly and without changing anything in our business logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_document_store = QdrantDocumentStore(\":memory:\", embedding_dim=384)\n",
    "qdrant_indexing_pipeline = build_indexing_pipline(qdrant_document_store)\n",
    "qdrant_indexing_pipeline.run({\"doc_embedder\": {\"documents\": documents}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_document_store.count_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_rag_pipe = build_openai_rag_pipeline(\n",
    "    QdrantEmbeddingRetriever(qdrant_document_store), template\n",
    ")\n",
    "qdrant_response = qdrant_rag_pipe.run(\n",
    "    {\"embedder\": {\"text\": query}, \"prompt_builder\": {\"question\": query}}\n",
    ")\n",
    "pprint(qdrant_response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "924722e6"
   },
   "source": [
    "# Understanding Tokens and Byte Pair Encoding (BPE) with `tiktoken`\n",
    "\n",
    "This notebook explains the concepts of tokens and Byte Pair Encoding (BPE), which are fundamental to how large language models process text. We'll use the `tiktoken` library, developed by OpenAI, to demonstrate these concepts.\n",
    "\n",
    "## What are Tokens?\n",
    "\n",
    "In the context of language models, a token is a sequence of characters that the model treats as a single unit. This can be a word, part of a word, or even punctuation. Models don't process raw text character by character; instead, they break down text into tokens.\n",
    "\n",
    "## Why Tokenization?\n",
    "\n",
    "Tokenization is crucial for several reasons:\n",
    "\n",
    "* **Efficiency:** Processing tokens is much more efficient than processing individual characters, especially for large amounts of text.\n",
    "* **Vocabulary:** Models work with a fixed vocabulary of tokens. Tokenization maps the input text to this vocabulary.\n",
    "* **Handling Out-of-Vocabulary Words:** BPE, which we'll discuss next, helps handle words that are not explicitly in the model's vocabulary by breaking them down into sub-word units.\n",
    "\n",
    "## What is Byte Pair Encoding (BPE)?\n",
    "\n",
    "Byte Pair Encoding (BPE) is a common tokenization algorithm used by many language models, including those from OpenAI. It works by iteratively merging the most frequent pairs of bytes (or characters) in a text until a desired vocabulary size is reached.\n",
    "\n",
    "Here's a simplified idea of how it works:\n",
    "\n",
    "1. Start with a vocabulary of individual characters.\n",
    "2. Count the frequency of adjacent pairs of characters.\n",
    "3. Replace the most frequent pair with a new, unique token.\n",
    "4. Repeat steps 2 and 3 until a desired number of tokens is created or no further merges are beneficial.\n",
    "\n",
    "This process creates a vocabulary of tokens that includes individual characters, common words, and sub-word units.\n",
    "\n",
    "## Simplified BPE from Scratch\n",
    "\n",
    "To better understand BPE, let's implement a simplified version from scratch. This implementation will demonstrate the core idea of iteratively merging the most frequent pairs of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ff696a8",
    "outputId": "07eda8fb-37a2-4a6a-c90a-76f474b84c2c"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_pair_counts(corpus: list[list[str]]) -> Counter:\n",
    "    \"\"\"Count all adjacent character pairs across the corpus.\"\"\"\n",
    "    pairs = [(word[i], word[i + 1]) for word in corpus for i in range(len(word) - 1)]\n",
    "    return Counter(pairs)\n",
    "\n",
    "\n",
    "def merge_pair(corpus: list[list[str]], pair: tuple[str, str]) -> list[list[str]]:\n",
    "    \"\"\"Merge all occurrences of a pair into a single token.\"\"\"\n",
    "    a, b = pair\n",
    "    merged_token = a + b\n",
    "\n",
    "    new_corpus = []\n",
    "    for word in corpus:\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1 and word[i] == a and word[i + 1] == b:\n",
    "                new_word.append(merged_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_corpus.append(new_word)\n",
    "\n",
    "    return new_corpus\n",
    "\n",
    "\n",
    "# --- Training loop ---\n",
    "\n",
    "corpus: list[list[str]] = [\n",
    "    [\"l\", \"o\", \"w\", \"</w>\"],\n",
    "    [\"l\", \"o\", \"w\", \"e\", \"r\", \"</w>\"],\n",
    "    [\"l\", \"o\", \"w\", \"e\", \"s\", \"t\", \"</w>\"],\n",
    "    [\"s\", \"l\", \"o\", \"w\", \"</w>\"],\n",
    "    [\"s\", \"l\", \"o\", \"w\", \"e\", \"r\", \"</w>\"],\n",
    "]\n",
    "\n",
    "vocab: set[str] = set(ch for word in corpus for ch in word)\n",
    "\n",
    "num_merges: int = 3\n",
    "for step in range(num_merges):\n",
    "    pair_counts = get_pair_counts(corpus)\n",
    "    best_pair = pair_counts.most_common(1)[0][0]\n",
    "    corpus = merge_pair(corpus, best_pair)\n",
    "    vocab.add(best_pair[0] + best_pair[1])\n",
    "\n",
    "    print(f\"Step {step + 1}: merged {best_pair} -> {best_pair[0] + best_pair[1]}\")\n",
    "    print(f\"Current corpus: {corpus}\")\n",
    "    print(f\"Current vocab: {vocab}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3867458"
   },
   "source": [
    "Now, let's go back to using the `tiktoken` library to explore its functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1784e95d"
   },
   "source": [
    "## Using `tiktoken`\n",
    "\n",
    "`tiktoken` is a fast open-source tokenizer by OpenAI. It's used to count tokens and encode/decode text using various encodings used by their models.\n",
    "\n",
    "First, let's install `tiktoken`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b07f346",
    "outputId": "5edf97e7-2520-4573-ccdf-21df2cf7fae4"
   },
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77dd933b"
   },
   "source": [
    "Now, let's import `tiktoken` and explore some of its functionalities.\n",
    "\n",
    "`tiktoken` supports different encodings for different models. You can get an encoding by its name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "181ada77",
    "outputId": "c27b2a70-9836-4687-b875-739fac637ffc"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Get the encoding for a specific model (e.g., 'gpt-4')\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "# Alternatively, get an encoding by its name\n",
    "# encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(f\"Encoding name: {encoding.name}\")\n",
    "print(f\"Vocabulary size: {encoding.n_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75cc4feb"
   },
   "source": [
    "Let's see how to encode text into tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d24628f",
    "outputId": "2fa2e704-1c59-41c4-c0a4-0f819b9978f0"
   },
   "outputs": [],
   "source": [
    "text = \"Hello, world! This is a test sentence.\"\n",
    "\n",
    "# Encode the text\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf6dc6c1"
   },
   "source": [
    "You can also decode tokens back into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f0b6951",
    "outputId": "8cf582ec-8dd7-4d58-ace1-3805ca656ffd"
   },
   "outputs": [],
   "source": [
    "# Decode the tokens\n",
    "decoded_text = encoding.decode(tokens)\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Decoded text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f910e08"
   },
   "source": [
    "`tiktoken` is particularly useful for counting tokens, which is important for estimating costs when using language models or managing context window limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b615c09c",
    "outputId": "b2078291-ec07-48c0-c670-e27a37600682"
   },
   "outputs": [],
   "source": [
    "text_to_count = \"This is a longer piece of text to demonstrate token counting.\"\n",
    "num_tokens = len(encoding.encode(text_to_count))\n",
    "print(f\"Text: '{text_to_count}'\")\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2716b750"
   },
   "source": [
    "## How BPE Affects Tokenization\n",
    "\n",
    "Let's look at some examples to see how BPE can break down words into sub-word units. Notice how common words or parts of words might be single tokens, while less common words or combinations might be split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cac8f8c7",
    "outputId": "17616f67-4fdb-48de-b33d-42c56dcadffe"
   },
   "outputs": [],
   "source": [
    "words = [\"tokenization\", \"untokenizable\", \"jupyter\", \"notebook\", \"extraordinarily\"]\n",
    "\n",
    "for word in words:\n",
    "    tokens = encoding.encode(word)\n",
    "    print(f\"Word: '{word}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Decoded tokens: {[encoding.decode([token]) for token in tokens]}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0ac281c"
   },
   "source": [
    "As you can see, \"tokenization\", \"jupyter\", and \"notebook\" are treated as single tokens in this encoding. \"untokenizable\" is split into \"un\" and \"tokenizable\", and \"extraordinarily\" is split into several sub-word units. This demonstrates how BPE can handle variations in words and less common terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e5decd1"
   },
   "source": [
    "### Handling Special Tokens\n",
    "\n",
    "Special tokens are used for specific purposes by language models. We can see how `tiktoken` handles them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb5333ec",
    "outputId": "e8f3ca43-8e9c-4e85-d2dd-a4bf2daa35d6"
   },
   "outputs": [],
   "source": [
    "text_with_special_tokens = (\n",
    "    \"<|endoftext|> This is a test with a special token. <|fim_middle|>\"\n",
    ")\n",
    "\n",
    "# Encode the text\n",
    "tokens_with_special = encoding.encode(text_with_special_tokens, allowed_special=\"all\")\n",
    "\n",
    "print(f\"Original text with special tokens: {text_with_special_tokens}\")\n",
    "print(f\"Tokens: {tokens_with_special}\")\n",
    "print(f\"Number of tokens: {len(tokens_with_special)}\")\n",
    "\n",
    "# Decode the tokens\n",
    "decoded_text_with_special = encoding.decode(tokens_with_special)\n",
    "print(f\"Decoded text with special tokens: {decoded_text_with_special}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a17c9fe"
   },
   "source": [
    "### Handling Typos and Nonexistent Words\n",
    "\n",
    "Let's see how `tiktoken` tokenizes words that are not in its vocabulary, such as typos or made-up words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ad2d056",
    "outputId": "056c7a8b-93dd-43e6-d598-7027fd212acb"
   },
   "outputs": [],
   "source": [
    "typo_text = \"This is a typoed sentense with a nonexistentword.\"\n",
    "\n",
    "# Encode the text\n",
    "typo_tokens = encoding.encode(typo_text)\n",
    "\n",
    "print(f\"Original text with typo: {typo_text}\")\n",
    "print(f\"Tokens: {typo_tokens}\")\n",
    "print(f\"Decoded tokens: {[encoding.decode([token]) for token in typo_tokens]}\")\n",
    "print(f\"Number of tokens: {len(typo_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24887c5f"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Understanding tokenization and BPE is fundamental to working with large language models. `tiktoken` provides a convenient way to work with tokenization for OpenAI models, allowing you to encode/decode text and count tokens efficiently. This knowledge is essential for managing model inputs, understanding model outputs, and estimating the computational resources required for processing text."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

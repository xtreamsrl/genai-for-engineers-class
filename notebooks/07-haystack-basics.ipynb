{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Haystack?\n",
    "Haystack is an open source framework for building production-ready LLM applications, retrieval-augmented generative pipelines and state-of-the-art search systems that work intelligently over large document collections. It lets you quickly try out the latest AI models while being flexible and easy to use. Our inspiring community of users and builders has helped shape Haystack into the modular, intuitive, complete framework it is today.\n",
    "\n",
    "Haystack offers comprehensive tooling for developing state-of-the-art AI systems that use LLMs.\n",
    "- Use models hosted on platforms like Hugging Face, OpenAI, Cohere, Mistral, and more.\n",
    "- Use models deployed on SageMaker, Bedrock, Azureâ€¦\n",
    "- Take advantage of our document stores: OpenSearch, Pinecone, Weaviate, QDrant and more.\n",
    "- Our growing ecosystem of community integrations provide tooling for evaluation, monitoring, data ingestion and every layer of your LLM application.\n",
    "\n",
    "From https://haystack.deepset.ai/overview/intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite: Get the OpenAI Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWRZeaVJS3LA",
    "outputId": "24292119-8a0f-49de-f137-a18eb0dcdb47"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if not importlib.util.find_spec(\"class_utils\"):\n",
    "    !pip install -qqq git+https://github.com/xtreamsrl/genai-for-engineers-class\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's chat\n",
    "\n",
    "Let's create a very simple chat with the facilities provided by Haystack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWeZFFXAVzup"
   },
   "outputs": [],
   "source": [
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage.from_system(\n",
    "        \"Always respond in Italian even if some input data is in other languages.\"\n",
    "    ),\n",
    "    ChatMessage.from_user(\"What's Natural Language Processing? Be brief.\"),\n",
    "]\n",
    "\n",
    "openai_chat_generator = OpenAIChatGenerator(model=\"gpt-4.1\")\n",
    "openai_response = openai_chat_generator.run(messages=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check out the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(openai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic\n",
    "Now, the value of frameworks like Haystack is accelerating the development.\n",
    "\n",
    "Can you switch from OpenAI to Anthropic in less than 5 minutes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ANTHROPIC_API_KEY\"] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_response = ...\n",
    "pprint(anthropic_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiteLLM and Others\n",
    "\n",
    "The same result can be obtained by using LiteLLM or other tools. Nowadays, many vendors, inclusing Google with Gemini and Mistral, are directly supporting the OpenAI SDK as their own client. It is as simple as changing the base url in the SDK settings.\n",
    "\n",
    "Let's see how litellm works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "response = litellm.completion(...)  # run with openai\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = litellm.completion(...)  # run with anthropic\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

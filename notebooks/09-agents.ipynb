{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM-powered agents are specialized instances of language models designed to perform specific tasks autonomously. \n",
    "\n",
    "Multi-agent collaboration can outperform a single prompt due to several key advantages.\n",
    "\n",
    "### Complexity Management\n",
    "\n",
    "LLM-powered agents can break down complex tasks into smaller, manageable sub-tasks. Each agent focuses on a specific part, leading to more efficient and accurate processing.\n",
    "\n",
    "### Specialization\n",
    "\n",
    "Different agents can be fine-tuned or designed for particular domains, resulting in expert-level performance in those areas, whereas a single prompt relies on a general-purpose model that may lack depth in specific fields.\n",
    "\n",
    "### Parallelism\n",
    "\n",
    "Agents can operate simultaneously on different tasks, significantly speeding up the overall process compared to the sequential nature of handling tasks with a single prompt.\n",
    "\n",
    "### Scalability\n",
    "\n",
    "A multi-agent system can easily scale by adding new agents to manage additional tasks, distributing the workload more effectively than trying to scale a single model.\n",
    "\n",
    "### Redundancy and Error Handling\n",
    "\n",
    "Multiple agents provide redundancy, enhancing reliability. If one agent fails or performs suboptimally, others can compensate, ensuring the overall task is still completed effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: packages and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWRZeaVJS3LA",
    "outputId": "24292119-8a0f-49de-f137-a18eb0dcdb47"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import autogen\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating agents\n",
    "\n",
    "For this purpose, we will use the latest model, GPT-4o.\n",
    "\n",
    "Less advanced models are less effective in writing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"config_list\": [{\"model\": \"gpt-4o\"}],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Proxy\n",
    "UserProxyAgent is a special type of agent that can be used to proxy user input to another agent or group of agents. It supports the following human input modes:\n",
    "- ALWAYS: Always ask user for input.\n",
    "- NEVER: Never ask user for input. In this mode, the agent will use the default response (if any) to respond to the message. Or using underlying LLM model to generate response if provided.\n",
    "- AUTO: Only ask user for input when conversation is terminated by the other agent(s). Otherwise, use the default response (if any) to respond to the message. Or using underlying LLM model to generate response if provided.\n",
    "\n",
    "We will execute the code natively without docker. This is a security flaw, and in real life it should be avoided, as the execution of arbitrary code can expose sensitive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"dataviz_agents\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    "    human_input_mode=\"NEVER\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coder, Software Engineer, and Data Visualization specialist\n",
    "\n",
    "Next we define our agents according to their role.\n",
    "\n",
    "In essence, an agent is definied by its system prompt. Then, it will take part in the conversation as if it was a human being.\n",
    "\n",
    "We will define:\n",
    "- a coder, taked with developing the python code to perform a task,\n",
    "- a software engineer, that will criticise the code written by the coder, and suggest improvements, and\n",
    "- a data visualization specialist, that should evaluate the quality of the visualization.\n",
    "\n",
    "Once the chat is triggered, the agents should collaborate autonomously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Coder\",\n",
    "    description=\"Coder\",\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_eng = autogen.AssistantAgent(\n",
    "    name=\"Software_Engineer\",\n",
    "    description=\"Software Engineer\",\n",
    "    system_message=...,\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataviz_specialist = autogen.AssistantAgent(\n",
    "    name=\"Data_Visualization_Specialist\",\n",
    "    description=\"Data Visualization Specialist\",\n",
    "    system_message=...,\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, coder, dataviz_specialist, sw_eng], messages=[], max_round=20\n",
    ")\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the chat\n",
    "Let's trigger the grup chat and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://raw.githubusercontent.com/uwdata/draco/master/data/cars.csv\"\n",
    "query = \"plot a visualization that tells us about the relationship between weight and horsepower\"\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=f\"download data from {link} and use it to plot something that reply to the following query: {query}. \"\n",
    "    f\"Save the plot to a file named results.png\",\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if not importlib.util.find_spec(\"utils\"):\n",
    "    !pip install -qqq git+https://github.com/xtreamsrl/genai-for-engineers-class\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ...  # Insert your OpenAI key here\n",
    "os.environ[\"GUARDRAILS_API_KEY\"] = ...  # Insert your Guardrails AI key here\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The issue with users\n",
    "Text is a wonderful interfaceâ€”versatile, flexible, and universal. However, it is also very difficult to control.\n",
    "\n",
    "If you are developing customer-facing GenAI applications, you must take security seriously.\n",
    "\n",
    "Your application will be vulnerable to various types of attacks, including prompt injections, malicious requests, and attempts to force data or prompt leakage.\n",
    "\n",
    "**Prompt Injection Attacks**: Attackers can craft inputs that manipulate the model's behavior in unintended ways. This can lead to the model generating harmful, misleading, or sensitive information. To mitigate this risk, developers can implement strict input validation, sanitize user inputs, and use context-aware filtering to detect and block suspicious patterns.\n",
    "\n",
    "**Malicious Requests**: Adversaries may send requests designed to exploit vulnerabilities within the model or the surrounding infrastructure. To defend against such threats, it's essential to incorporate robust security measures such as rate limiting, authentication, and anomaly detection.\n",
    "\n",
    "**Data Leakage**: Large language models can inadvertently reveal sensitive information contained within their training data or prompt history. Implementing techniques like differential privacy, which introduces noise to the data, and ensuring that training data is anonymized can help minimize this risk.\n",
    "\n",
    "LLM security is an open problem with no definitive solution, but implementing guardrails can help. Establishing guardrails involves setting boundaries for the modelâ€™s behavior, defining acceptable use cases, implementing safety protocols, and continuously monitoring the modelâ€™s outputs for signs of misuse or deviation from expected norms. Automated systems can flag and review suspicious activity in real time.\n",
    "\n",
    "Let's see how our simple RAG pipeline can be subject to such attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: packages and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWRZeaVJS3LA",
    "outputId": "24292119-8a0f-49de-f137-a18eb0dcdb47"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
    "from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever\n",
    "\n",
    "from utils.data import get_movie_dataset_as_documents\n",
    "from utils.haystack_pipelines import (\n",
    "    build_indexing_pipline,\n",
    "    build_openai_rag_pipeline,\n",
    ")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the RAG pipeline\n",
    "\n",
    "We extracted the same functions used in notebook 05, so that we can focus on the issue of security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = get_movie_dataset_as_documents(100)\n",
    "documents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = QdrantDocumentStore(\":memory:\", embedding_dim=384)\n",
    "indexing_pipeline = build_indexing_pipline(document_store)\n",
    "indexing_pipeline.run({\"doc_embedder\": {\"documents\": documents}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the questions based on the given context.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "Question: {{ question }}\n",
    "Answer:\n",
    "\"\"\"\n",
    "rag_pipe = build_openai_rag_pipeline(QdrantEmbeddingRetriever(document_store), template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Nazi user?\n",
    "Now, for the sake of the argument let us assume that our use case forbids that the user can ever ask something related to Nazism.\n",
    "\n",
    "But what if a user does? Is out system robust?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nazi_query = \"What film talks about Adolf Hitler?\"\n",
    "\n",
    "nazi_response = rag_pipe.run(\n",
    "    {\"embedder\": {\"text\": nazi_query}, \"prompt_builder\": {\"question\": nazi_query}}\n",
    ")\n",
    "print(\"Without Guardrails...\")\n",
    "pprint(nazi_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly not. But we can protect ourselves. \n",
    "\n",
    "Let's try and include a guardrail.\n",
    "\n",
    "Find out more on https://github.com/guardrails-ai/guardrails and https://hub.guardrailsai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!guardrails configure --disable-metrics --clear-token --enable-remote-inferencing --token $GUARDRAILS_API_KEY\n",
    "!guardrails hub install hub://guardrails/sensitive_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Disclaimer ðŸš¨ðŸš¨ðŸš¨\n",
    "Guardrails are a new concept and they are not mature. We are not advocating the usage of guardrails.ai or any other specific library. Feel free to handcraft your own guardrails with explicit prompts to LLMs.\n",
    "\n",
    "In case you want to moderate a conversation, you might be interested to check out [Llama Guard](https://huggingface.co/meta-llama/LlamaGuard-7b) by Meta and the [Moderation API](https://platform.openai.com/docs/guides/moderation) by OpenAI. \n",
    "\n",
    "Both tools use LLMs to detect harmful, violent or otherwise toxic language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guardrails import Guard, OnFailAction\n",
    "from guardrails.hub import SensitiveTopic\n",
    "\n",
    "print(\"\\nWith Guardrails...\")\n",
    "nazi_guard = Guard().use(\n",
    "    SensitiveTopic,\n",
    "    sensitive_topics=[\"nazism\", \"cat\"],\n",
    "    disable_classifier=False,\n",
    "    disable_llm=False,\n",
    "    on_fail=OnFailAction.NOOP,\n",
    ")\n",
    "nazi_guard.validate(nazi_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better. We see that the validation fails. In out real-world application, we could understand that the request is illigal and handle it somehow - possibly by refreaining from answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A malicious user?\n",
    "Now, what if our user wants to get some personal information which is present in our dataset - or has been learnt by our model?\n",
    "\n",
    "We must be particularly careful about this, because Deep Learning models are prone to memorise outliers - such as names, phone numbers, and email addresses - and regurgitating such information at inference time when promped to consider other unusual samples, such as repeating the same word forever.\n",
    "\n",
    "Here, a guardrail can help as well. There are multiple libraries to detect Personal Identifiable Information (PIIs), for instance [Presidio](https://github.com/microsoft/presidio/) by Microsoft. We can use a guardrail based on that to avoid sharing such information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_query = \"Find a great science-fiction movie. Then tell me the name and the email of a couple of actors.\"\n",
    "\n",
    "pii_response = rag_pipe.run(\n",
    "    {\"embedder\": {\"text\": pii_query}, \"prompt_builder\": {\"question\": pii_query}}\n",
    ")\n",
    "print(\"Without Guardrails...\")\n",
    "pprint(pii_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the right guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and use it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the guardrail would have prevented troubles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardrails with LLM and structured generation\n",
    "\n",
    "Most often, you do not use ad-hoc libraries such as guardrails, but you resort to prompts with general-purpose LLMs.\n",
    "\n",
    "In this case, it may be useful to have a structured output from the LLM, so that you can parse it and check if it contains any unwanted information.\n",
    "\n",
    "How would you craft a prompt to avoid users ask about personal information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "class PIICheck(BaseModel):\n",
    "    contains_pii: bool\n",
    "    pii_sentences: list[str]\n",
    "\n",
    "\n",
    "pii_text_to_check = pii_response[\"llm\"][\"replies\"][0]\n",
    "\n",
    "response = ...\n",
    "\n",
    "pii_check = PIICheck.model_validate_json(response.output_text)\n",
    "pii_check"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if not importlib.util.find_spec(\"utils\"):\n",
    "    !pip install -qqq git+https://github.com/xtreamsrl/genai-for-engineers-class\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ...  # Insert your OpenAI key here\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = ...  # Insert your Langfuse secret key here\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = ...  # Insert your Langfuse public key here\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Observability?\n",
    "Application observability is the capability to understand the internal state of a system based on the external outputs it produces, like logs, metrics, and traces. It enables deep insights into application behavior, performance, and health, helping to answer not just *what* is happening but *why* it is happening. \n",
    "\n",
    "Application observability has been a hot topic for years now and a mature ecosystem of tools exists to support \"normal\" applications. For instance, the `opentelemetry` standard and tooling is supported by all the major cloud vendors and by all the most modern web frameworks.\n",
    "\n",
    "#### The Three Pillars of Observability\n",
    "\n",
    "1. **Logs**: Detailed, timestamped records of discrete events. Used for tracking the flow of execution and debugging specific issues.\n",
    "2. **Metrics**: Quantitative data points, such as CPU usage, memory consumption, request rates, and error rates. Used for monitoring performance and system health.\n",
    "3. **Traces**: Records of the path a request takes through a distributed system. Used for identifying performance bottlenecks and understanding system dependencies.\n",
    "\n",
    "#### Benefits of Observability\n",
    "\n",
    "- **Rapid Troubleshooting**: Quickly identify and resolve issues by understanding their root cause.\n",
    "- **Performance Optimization**: Detect and address performance bottlenecks.\n",
    "- **Proactive Monitoring**: Anticipate and mitigate issues before they impact users.\n",
    "- **Improved Development Practices**: Gain immediate feedback on code changes.\n",
    "- **Enhanced User Experience**: Ensure high availability and reliability.\n",
    "\n",
    "#### Implementing Observability\n",
    "\n",
    "1. **Instrumentation**: Use libraries and tools to collect logs, metrics, and traces (e.g., OpenTelemetry).\n",
    "2. **Centralized Logging and Monitoring**: Aggregate data using platforms like ELK Stack, Prometheus, Grafana, and Jaeger.\n",
    "3. **Contextual Data**: Include metadata (e.g., request IDs, user IDs) for better correlation and analysis.\n",
    "4. **Alerting and Visualization**: Set up alerts and dashboards for real-time monitoring and visualization.\n",
    "5. **Continuous Improvement**: Regularly review observability practices and refine instrumentation and monitoring strategies.\n",
    "\n",
    "# Observability for the GenAI Era\n",
    "Observability is tricky for GenAI applications due to their inherent complexity and dynamic nature. These applications rely on intricate machine learning models that are often opaque (\"black boxes\"), making it challenging to interpret their internal states and decision-making processes. The high dimensionality of input data and the stochastic nature of model training and inference further complicate the task of monitoring and debugging. Additionally, GenAI systems typically involve a multitude of interdependent components, such as data pipelines, model serving infrastructure, and distributed computing resources, which generate vast amounts of heterogeneous data. This makes it difficult to correlate logs, metrics, and traces across different layers of the stack, hindering effective root cause analysis and performance optimization.\n",
    "\n",
    "Fortunately, in the last 2 years, some new tools have emerged to ease the task of observing GenAI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: packages and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWRZeaVJS3LA",
    "outputId": "24292119-8a0f-49de-f137-a18eb0dcdb47"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
    "from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever\n",
    "\n",
    "from utils.data import get_movie_dataset_as_documents\n",
    "from utils.haystack_pipelines import (\n",
    "    build_indexing_pipline,\n",
    "    build_prompt_building_pipeline,\n",
    ")\n",
    "from langfuse import get_client\n",
    "from langfuse.openai import OpenAI\n",
    "\n",
    "os.environ[\"HAYSTACK_CONTENT_TRACING_ENABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our usual indexing pipeline without any modification. We're not interested in observing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = get_movie_dataset_as_documents(100)\n",
    "document_store = QdrantDocumentStore(\":memory:\", embedding_dim=384)\n",
    "indexing_pipeline = build_indexing_pipline(document_store)\n",
    "indexing_pipeline.run({\"doc_embedder\": {\"documents\": documents}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Registry\n",
    "\n",
    "Prompts are a fundamental component of code, and as such, they should be versioned accordingly.\n",
    "\n",
    "However, there are scenarios where you may want to perform A/B testing on prompts over the air or update the prompt without deploying the entire application. An example of this approach can be seen in the Touring app: [Touring](https://touringapp.eu/).\n",
    "\n",
    "A remote prompt registry offers a great solution for these needs.\n",
    "\n",
    "A prompt registry functions similarly to a model registry, like the [MLFlow Registry](https://mlflow.org/docs/latest/model-registry.html), but it is specifically designed for prompts. In a prompt registry, you can store and compare different versions of the same prompt, revert to previous versions, and update the deployed prompt without altering the applicationâ€™s code.\n",
    "\n",
    "For both prompt registry and LLM observability, we will use [Langfuse](https://langfuse.com/). However, numerous similar tools are emerging. Notably, the [openllmetry](https://github.com/traceloop/openllmetry) standard is gaining popularity in this space.\n",
    "\n",
    "By using a prompt registry, you can efficiently manage, compare, and deploy prompts, ensuring a streamlined and flexible approach to prompt versioning and observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse = get_client()\n",
    "template = ...\n",
    "template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we've sourced our prompt from the prompt registry. Now let's run the pipeline until the full prompt is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_building_pipe = build_prompt_building_pipeline(\n",
    "    QdrantEmbeddingRetriever(document_store), template\n",
    ")\n",
    "\n",
    "query = \"What film talks about the atomic bomb?\"\n",
    "prompt = ...\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll use the `openai` client to run the query to the LLM.\n",
    "Haystack has a native integration with Langfuse, but it is a bit buggy at the moment.\n",
    "\n",
    "Note how we import the `openai` client ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()\n",
    "\n",
    "# get a response from openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After runnig this notebook, check out your account on Langfuse and analyze the traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observability with Haystack\n",
    "Altough some data are not logged correctly, let's try and instrument the full pipeline with Langfuse.\n",
    "Pay attention to enable tracing with the proper environment variable. Check out the documentation: https://haystack.deepset.ai/integrations/langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HAYSTACK_CONTENT_TRACING_ENABLED\"] = \"true\"\n",
    "\n",
    "# build and instrument the rag pipeline"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

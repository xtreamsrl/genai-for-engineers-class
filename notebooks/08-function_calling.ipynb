{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "if not importlib.util.find_spec(\"class_utils\"):\n",
    "    !pip install -qqq git+https://github.com/xtreamsrl/genai-for-engineers-class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function calling for language models (LLMs) allows developers to define and invoke specific functions within the modelâ€™s output. \n",
    "\n",
    "In an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call one or many functions. \n",
    "\n",
    "The LLM **does not call** the function; instead, the model generates JSON that you can use to call the function in your code.\n",
    "\n",
    "This feature enhances the precision and control over the responses generated by the LLM, enabling more accurate and relevant outputs. By specifying a function, developers can ensure the LLM processes tasks such as data extraction, content generation, or API interactions in a structured and predictable manner. \n",
    "\n",
    "This is especially useful in applications where consistency and correctness are critical, such as automated customer support, dynamic content generation, and complex data manipulation. \n",
    "\n",
    "Overall, function calling transforms LLMs from simple text generators into versatile tools that can execute predefined tasks, improving reliability and efficiency in various development scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: packages and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWRZeaVJS3LA",
    "outputId": "24292119-8a0f-49de-f137-a18eb0dcdb47"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import httpx\n",
    "\n",
    "from haystack import Document\n",
    "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
    "from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever\n",
    "\n",
    "from class_utils.data import get_movie_dataset_as_documents\n",
    "from class_utils.haystack_pipelines import (\n",
    "    build_indexing_pipline,\n",
    "    build_prompt_building_pipeline,\n",
    ")\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ...\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "THE_MOVIE_DB_BEARER = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "\n",
    "Let's run our usual indexing pipeline without any modification.\n",
    "\n",
    "However, this time we only load 5 documents, because we want the query to find no matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = get_movie_dataset_as_documents(5)\n",
    "document_store = QdrantDocumentStore(\":memory:\", embedding_dim=384)\n",
    "indexing_pipeline = build_indexing_pipline(document_store)\n",
    "indexing_pipeline.run({\"doc_embedder\": {\"documents\": documents}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also hard-code the template, to focus on the function calling and not on other topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the questions based on the given context.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "Question: {{ question }}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run the pipeline. Please pay attention to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_building_pipe = build_prompt_building_pipeline(\n",
    "    QdrantEmbeddingRetriever(document_store), template\n",
    ")\n",
    "\n",
    "query = \"What film talks about the atomic bomb? If you find no matching movies in the context, use your tools and functions to search for some.\"\n",
    "prompt_builder_output = prompt_building_pipe.run(\n",
    "    {\"embedder\": {\"text\": query}, \"prompt_builder\": {\"question\": query}}\n",
    ")\n",
    "prompt = prompt_builder_output.get(\"prompt_builder\").get(\"prompt\")\n",
    "pprint(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no movies specifically about the atomic bomb.\n",
    "\n",
    "However, there are countless other movies out there. If only our language model could access an external service...\n",
    "\n",
    "It turn out it can! OpenAI supports Function Calling: https://platform.openai.com/docs/guides/function-calling.\n",
    "\n",
    "We can provide the model with some definitions of functions and let the model decide whether to call them.\n",
    "\n",
    "We must note that the model **DOES NOT** execute the function itself, that is up to us. The model just answer with an indication of the function to execute and the parameters it would use.\n",
    "\n",
    "Let's try and make GPT aware of our function `search_for_movies` below. We should not ask the model to choose the value of the bearer token, as we will use our own key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring openai for function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "messages = ...\n",
    "tools = ...\n",
    "response = client.responses.create(\n",
    "    input=messages, model=\"gpt-4.1\", parallel_tool_calls=False, tools=tools\n",
    ")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the implementation of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_movies(query: str, bearer_token: str) -> list[Document]:\n",
    "    response = httpx.get(\n",
    "        url=\"https://api.themoviedb.org/3/search/movie\",\n",
    "        params={\n",
    "            \"include_adult\": False,\n",
    "            \"language\": \"en-US\",\n",
    "            \"query\": query,\n",
    "            \"region\": \"US\",\n",
    "        },\n",
    "        headers={\n",
    "            \"accept\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {bearer_token}\",\n",
    "        },\n",
    "    )\n",
    "    docs = [\n",
    "        Document(\n",
    "            id=movie[\"id\"],\n",
    "            content=f\"title: {movie['original_title']} \\noverview: {movie['overview']}\",\n",
    "            meta={\n",
    "                \"title\": movie.get(\"original_title\"),\n",
    "                \"release_date\": movie.get(\"release_date\"),\n",
    "            },\n",
    "        )\n",
    "        for movie in response.json()[\"results\"]\n",
    "    ]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing the actual function call\n",
    "Now we must interpret the directions of the model. \n",
    "\n",
    "If it wants to call our function, we will extract the arguments and the execute `search_for_movies` with them.\n",
    "\n",
    "Then, we will properly format the return values in a new prompt and we will pass it to the model. \n",
    "\n",
    "This should allow GPT to answer our initial query.\n",
    "\n",
    "We will not do it, but please consider validating and sanitising the arguments to be passed to your functions. The LLMs may product invalid inputs, that may crash your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the response to get an answer to the original query!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
